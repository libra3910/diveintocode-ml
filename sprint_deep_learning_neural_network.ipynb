{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "sprint_deep_learning_neural_network.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/libra3910/diveintocode-ml/blob/master/sprint_deep_learning_neural_network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBrt_1fbtIgL"
      },
      "source": [
        "## sprint 深層学習スクラッチ ニューラルネットワーク"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dGx3Bk5tIgO"
      },
      "source": [
        "# データセットをダウンロードするコード\n",
        "from keras.datasets import mnist\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Btbv3ectIgQ"
      },
      "source": [
        "import numpy as np\n",
        "np.save(\"C:/Users/es/Documents/Python Scripts/3.Mar/FrameWork1/X_train\", X_train)\n",
        "np.save(\"C:/Users/es/Documents/Python Scripts/3.Mar/FrameWork1/y_train\", y_train)\n",
        "np.save(\"C:/Users/es/Documents/Python Scripts/3.Mar/FrameWork1/X_test\", X_test)\n",
        "np.save(\"C:/Users/es/Documents/Python Scripts/3.Mar/FrameWork1/y_test\", y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrtIn8yFtIgQ",
        "outputId": "e97a5159-fa43-4c06-873e-f5d162f9d67b"
      },
      "source": [
        "# データセットの確認\n",
        "print(X_train.shape) # (60000, 28, 28)\n",
        "print(X_test.shape) # (10000, 28, 28)\n",
        "print(y_train.shape) # (60000,)\n",
        "print(y_test.shape) # (10000,)\n",
        "print(X_train[0].dtype) # uint8\n",
        "print(X_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n",
            "(60000,)\n",
            "(10000,)\n",
            "uint8\n",
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3  18  18  18 126 136\n",
            "  175  26 166 255 247 127   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  30  36  94 154 170 253 253 253 253 253\n",
            "  225 172 253 242 195  64   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  49 238 253 253 253 253 253 253 253 253 251\n",
            "   93  82  82  56  39   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0  18 219 253 253 253 253 253 198 182 247 241\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  80 156 107 253 253 205  11   0  43 154\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0  14   1 154 253  90   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0 139 253 190   2   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0  11 190 253  70   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  35 241 225 160 108   1\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0  81 240 253 253 119\n",
            "   25   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  45 186 253 253\n",
            "  150  27   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  16  93 252\n",
            "  253 187   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 249\n",
            "  253 249  64   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  46 130 183 253\n",
            "  253 207   2   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  39 148 229 253 253 253\n",
            "  250 182   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0  24 114 221 253 253 253 253 201\n",
            "   78   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  23  66 213 253 253 253 253 198  81   2\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0  18 171 219 253 253 253 253 195  80   9   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0  55 172 226 253 253 253 253 244 133  11   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0 136 253 253 253 212 135 132  16   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAMtq02xtIgS"
      },
      "source": [
        "# 平滑化\n",
        "X_train = X_train.reshape(-1, 784)\n",
        "X_test = X_test.reshape(-1, 784)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fphheGrtIgT",
        "outputId": "73f8b004-ba15-46bc-c063-5c48efce31af"
      },
      "source": [
        "# 画像データの可視化\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "index = 2\n",
        "image = X_train[index].reshape(28,28)\n",
        "# X_train[index]: (784,)\n",
        "# image: (28, 28)\n",
        "plt.imshow(image, 'gray')\n",
        "plt.title('label : {}'.format(y_train[index]))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciEaORQctIgT",
        "outputId": "8728e707-fd9a-4a19-d9f8-fc8919fb8fae"
      },
      "source": [
        "# 画像データの可視化 float型\n",
        "import numpy as np\n",
        "index = 0\n",
        "image = X_train[index].reshape(28,28)\n",
        "image = image.astype(np.float) # float型に変換\n",
        "image -= 105.35 # 意図的に負の小数値を作り出してみる\n",
        "plt.imshow(image, 'gray')\n",
        "plt.title('label : {}'.format(y_train[index]))\n",
        "plt.show()\n",
        "print(image) # 値を確認"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[[-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -102.35  -87.35  -87.35  -87.35   20.65   30.65\n",
            "    69.65  -79.35   60.65  149.65  141.65   21.65 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -75.35\n",
            "   -69.35  -11.35   48.65   64.65  147.65  147.65  147.65  147.65  147.65\n",
            "   119.65   66.65  147.65  136.65   89.65  -41.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -56.35  132.65\n",
            "   147.65  147.65  147.65  147.65  147.65  147.65  147.65  147.65  145.65\n",
            "   -12.35  -23.35  -23.35  -49.35  -66.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -87.35  113.65\n",
            "   147.65  147.65  147.65  147.65  147.65   92.65   76.65  141.65  135.65\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -25.35\n",
            "    50.65    1.65  147.65  147.65   99.65  -94.35 -105.35  -62.35   48.65\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "   -91.35 -104.35   48.65  147.65  -15.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35   33.65  147.65   84.65 -103.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35  -94.35   84.65  147.65  -35.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35  -70.35  135.65  119.65   54.65    2.65 -104.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35  -24.35  134.65  147.65  147.65   13.65\n",
            "   -80.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35  -60.35   80.65  147.65  147.65\n",
            "    44.65  -78.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -89.35  -12.35  146.65\n",
            "   147.65   81.65 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  143.65\n",
            "   147.65  143.65  -41.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35  -59.35   24.65   77.65  147.65\n",
            "   147.65  101.65 -103.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35  -66.35   42.65  123.65  147.65  147.65  147.65\n",
            "   144.65   76.65 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35  -81.35    8.65  115.65  147.65  147.65  147.65  147.65   95.65\n",
            "   -27.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -82.35\n",
            "   -39.35  107.65  147.65  147.65  147.65  147.65   92.65  -24.35 -103.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -87.35   65.65  113.65\n",
            "   147.65  147.65  147.65  147.65   89.65  -25.35  -96.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35  -50.35   66.65  120.65  147.65  147.65\n",
            "   147.65  147.65  138.65   27.65  -94.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35   30.65  147.65  147.65  147.65  106.65\n",
            "    29.65   26.65  -89.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5Y3ufX5tIgU",
        "outputId": "60d1eb6c-4589-44e7-fa2e-374d72e36ace"
      },
      "source": [
        "index = 0\n",
        "image = X_train[index].reshape(28,28)\n",
        "image = image.astype(np.float) # float型に変換\n",
        "image -= 105.35 # 意図的に負の小数値を作り出してみる\n",
        "plt.imshow(image, 'gray', vmin = 0, vmax = 255)\n",
        "plt.title('label : {}'.format(y_train[index]))\n",
        "plt.show()\n",
        "print(image) # 値を確認"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[[-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -102.35  -87.35  -87.35  -87.35   20.65   30.65\n",
            "    69.65  -79.35   60.65  149.65  141.65   21.65 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -75.35\n",
            "   -69.35  -11.35   48.65   64.65  147.65  147.65  147.65  147.65  147.65\n",
            "   119.65   66.65  147.65  136.65   89.65  -41.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -56.35  132.65\n",
            "   147.65  147.65  147.65  147.65  147.65  147.65  147.65  147.65  145.65\n",
            "   -12.35  -23.35  -23.35  -49.35  -66.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -87.35  113.65\n",
            "   147.65  147.65  147.65  147.65  147.65   92.65   76.65  141.65  135.65\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -25.35\n",
            "    50.65    1.65  147.65  147.65   99.65  -94.35 -105.35  -62.35   48.65\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "   -91.35 -104.35   48.65  147.65  -15.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35   33.65  147.65   84.65 -103.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35  -94.35   84.65  147.65  -35.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35  -70.35  135.65  119.65   54.65    2.65 -104.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35  -24.35  134.65  147.65  147.65   13.65\n",
            "   -80.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35  -60.35   80.65  147.65  147.65\n",
            "    44.65  -78.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -89.35  -12.35  146.65\n",
            "   147.65   81.65 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  143.65\n",
            "   147.65  143.65  -41.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35  -59.35   24.65   77.65  147.65\n",
            "   147.65  101.65 -103.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35  -66.35   42.65  123.65  147.65  147.65  147.65\n",
            "   144.65   76.65 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35  -81.35    8.65  115.65  147.65  147.65  147.65  147.65   95.65\n",
            "   -27.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -82.35\n",
            "   -39.35  107.65  147.65  147.65  147.65  147.65   92.65  -24.35 -103.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35  -87.35   65.65  113.65\n",
            "   147.65  147.65  147.65  147.65   89.65  -25.35  -96.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35  -50.35   66.65  120.65  147.65  147.65\n",
            "   147.65  147.65  138.65   27.65  -94.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35   30.65  147.65  147.65  147.65  106.65\n",
            "    29.65   26.65  -89.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]\n",
            " [-105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35 -105.35\n",
            "  -105.35]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfYJ9SfZtIgU",
        "outputId": "1aee6d74-cbed-46e6-be50-6b1fbc52c2dc"
      },
      "source": [
        "# 前処理\n",
        "X_train = X_train.astype(np.float)\n",
        "X_test = X_test.astype(np.float)\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "print(X_train.max()) # 1.0\n",
        "print(X_train.min()) # 0.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUGvts5NtIgV",
        "outputId": "5ebc68f1-e479-418a-c7db-cfc668c95dab"
      },
      "source": [
        "y_train_one_hot = (y_train.reshape(-1,1) == np.arange(10)).astype(np.float64)\n",
        "y_test_one_hot = (y_test.reshape(-1,1) == np.arange(10)).astype(np.float64)\n",
        "print(y_train.shape)\n",
        "print(y_train_one_hot.shape)\n",
        "print(y_train_one_hot.dtype)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000,)\n",
            "(60000, 10)\n",
            "float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JbzOtEA1tIgV",
        "outputId": "e60cc9f2-ed9b-440d-c41f-0ae47b53c5e4"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
        "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
        "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
        "print(y_train.shape) # (60000,)\n",
        "print(y_train_one_hot.shape) # (60000, 10)\n",
        "print(y_train_one_hot.dtype) # float64"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000,)\n",
            "(60000, 10)\n",
            "float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hREtYwsAtIgW",
        "outputId": "be51de9c-726f-47bd-b1fc-4711b4898198"
      },
      "source": [
        "X_train.max(axis=1, keepdims=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.],\n",
              "       [1.],\n",
              "       [1.],\n",
              "       ...,\n",
              "       [1.],\n",
              "       [1.],\n",
              "       [1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvvV4cIGtIgW",
        "outputId": "67f0cb4e-ea82-4805-f73b-a58bf3c6f50d"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.01176471, 0.07058824, 0.07058824,\n",
              "       0.07058824, 0.49411765, 0.53333333, 0.68627451, 0.10196078,\n",
              "       0.65098039, 1.        , 0.96862745, 0.49803922, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.11764706, 0.14117647, 0.36862745, 0.60392157,\n",
              "       0.66666667, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.99215686, 0.88235294, 0.6745098 , 0.99215686, 0.94901961,\n",
              "       0.76470588, 0.25098039, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.19215686, 0.93333333,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.98431373, 0.36470588,\n",
              "       0.32156863, 0.32156863, 0.21960784, 0.15294118, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.07058824, 0.85882353, 0.99215686, 0.99215686,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.71372549,\n",
              "       0.96862745, 0.94509804, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.31372549, 0.61176471, 0.41960784, 0.99215686, 0.99215686,\n",
              "       0.80392157, 0.04313725, 0.        , 0.16862745, 0.60392157,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.05490196,\n",
              "       0.00392157, 0.60392157, 0.99215686, 0.35294118, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.54509804,\n",
              "       0.99215686, 0.74509804, 0.00784314, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.04313725, 0.74509804, 0.99215686,\n",
              "       0.2745098 , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.1372549 , 0.94509804, 0.88235294, 0.62745098,\n",
              "       0.42352941, 0.00392157, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.31764706, 0.94117647, 0.99215686, 0.99215686, 0.46666667,\n",
              "       0.09803922, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.17647059,\n",
              "       0.72941176, 0.99215686, 0.99215686, 0.58823529, 0.10588235,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.0627451 , 0.36470588,\n",
              "       0.98823529, 0.99215686, 0.73333333, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.97647059, 0.99215686,\n",
              "       0.97647059, 0.25098039, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.18039216, 0.50980392,\n",
              "       0.71764706, 0.99215686, 0.99215686, 0.81176471, 0.00784314,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.15294118,\n",
              "       0.58039216, 0.89803922, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.98039216, 0.71372549, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.09411765, 0.44705882, 0.86666667, 0.99215686, 0.99215686,\n",
              "       0.99215686, 0.99215686, 0.78823529, 0.30588235, 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.09019608, 0.25882353, 0.83529412, 0.99215686,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.77647059, 0.31764706,\n",
              "       0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.07058824, 0.67058824, 0.85882353,\n",
              "       0.99215686, 0.99215686, 0.99215686, 0.99215686, 0.76470588,\n",
              "       0.31372549, 0.03529412, 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.21568627, 0.6745098 ,\n",
              "       0.88627451, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.95686275, 0.52156863, 0.04313725, 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.53333333, 0.99215686, 0.99215686, 0.99215686,\n",
              "       0.83137255, 0.52941176, 0.51764706, 0.0627451 , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
              "       0.        , 0.        , 0.        , 0.        ])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wg_cPayVtIgW",
        "outputId": "067d3325-9d63-4b13-9143-1301307bff4c"
      },
      "source": [
        "def split_data(X, permutation, val_size_rate=0.2):\n",
        "    X = X[permutation]\n",
        "    val_size = int(len(X) * val_size_rate)\n",
        "    val = X[:val_size]\n",
        "    train = X[val_size:]\n",
        "    return train, val\n",
        "\n",
        "permutation = np.random.permutation(np.arange(len(X_train)))\n",
        "X_train, X_val = split_data(X_train, permutation)\n",
        "y_train, y_val = split_data(y_train, permutation)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(48000, 784)\n",
            "(48000,)\n",
            "(12000, 784)\n",
            "(12000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78lx0kQktIgX"
      },
      "source": [
        "#from sklearn.model_selection import train_test_split\n",
        "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
        "#print(X_train.shape) # (48000, 784)\n",
        "#print(X_val.shape) # (12000, 784)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzjcsahhtIgX"
      },
      "source": [
        "class GetMiniBatch:\n",
        "    \"\"\"\n",
        "    ミニバッチを取得するイテレータ\n",
        "    Parameters\n",
        "    ----------\n",
        "    X : 次の形のndarray, shape (n_samples, n_features)\n",
        "      訓練データ\n",
        "    y : 次の形のndarray, shape (n_samples, 1)\n",
        "      正解値\n",
        "    batch_size : int\n",
        "      バッチサイズ\n",
        "    seed : int\n",
        "      NumPyの乱数のシード\n",
        "    \"\"\"\n",
        "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
        "        self.batch_size = batch_size\n",
        "        np.random.seed(seed)\n",
        "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
        "        self._X = X[shuffle_index]\n",
        "        self._y = y[shuffle_index]\n",
        "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
        "    def __len__(self):\n",
        "        return self._stop\n",
        "    def __getitem__(self,item):\n",
        "        p0 = item*self.batch_size\n",
        "        p1 = item*self.batch_size + self.batch_size\n",
        "        return self._X[p0:p1], self._y[p0:p1]        \n",
        "    def __iter__(self):\n",
        "        self._counter = 0\n",
        "        return self\n",
        "    def __next__(self):\n",
        "        if self._counter >= self._stop:\n",
        "            raise StopIteration()\n",
        "        p0 = self._counter*self.batch_size\n",
        "        p1 = self._counter*self.batch_size + self.batch_size\n",
        "        self._counter += 1\n",
        "        return self._X[p0:p1], self._y[p0:p1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXUeTbfytIgY"
      },
      "source": [
        "class ScratchSimpleNeuralNetrowkClassifier():\n",
        "    \"\"\"\n",
        "    シンプルな三層ニューラルネットワーク分類器\n",
        "    Parameters\n",
        "    ----------\n",
        "    Attributes\n",
        "    ----------\n",
        "    batch_size = 20 # バッチサイズ\n",
        "    sigma = 0.01 # ガウス分布の標準偏差\n",
        "    n_features = 784 # 特徴量の数\n",
        "    n_nodes1 = 400 # 1層目のノード数\n",
        "    n_nodes2 = 200 # 2層目のノード数\n",
        "    n_output = 10 # 出力のクラス数（3層目のノード数）\n",
        "\n",
        "    \"\"\"\n",
        "    batch_size = 20\n",
        "    sigma = 0.01\n",
        "    n_features = 784\n",
        "    n_nodes1 = 400\n",
        "    n_nodes2 = 200\n",
        "    n_output = 10\n",
        "    \n",
        "    def __init__(self, seed=0, verbose = True, verbose2 = False):\n",
        "        self.seed = 0\n",
        "        self.verbose = verbose\n",
        "        self.verbose2 = verbose2\n",
        "        pass\n",
        "    \n",
        "    def _initalize_weight(self, n_features, n_nodes):\n",
        "        # 【問題1】重みの初期値を決めるコードの作成\n",
        "\n",
        "        W = self.sigma * np.random.randn(n_features, n_nodes)\n",
        "        \n",
        "        return W\n",
        "    \n",
        "    def _initalize_bias(self, n_features):\n",
        "        # 【問題1】バイアスの初期値を決めるコードの作成\n",
        "\n",
        "        B = self.sigma * np.random.randn(n_features)\n",
        "        \n",
        "        return B\n",
        "    \n",
        "    \n",
        "    def _initalizing(self):\n",
        "\n",
        "        np.random.seed(self.seed)\n",
        "        self.W1 = self._initalize_weight(self.n_features, self.n_nodes1) # W1: (784, 400)\n",
        "        self.W2 = self._initalize_weight(self.n_nodes1, self.n_nodes2) # W2: (400, 200) \n",
        "        self.W3 = self._initalize_weight(self.n_nodes2, self.n_output) # W3: (200, 10) \n",
        "\n",
        "        self.B1 = self._initalize_bias(self.n_nodes1) # B1: (400)\n",
        "        self.B2 = self._initalize_bias(self.n_nodes2) # B2: (200) \n",
        "        self.B3 = self._initalize_bias(self.n_output) # B3: (10) \n",
        "    \n",
        "    def _softmax(self, A):\n",
        "        # ソフトマックス関数\n",
        "        \n",
        "        if (A.ndim == 1):\n",
        "            A = Z[None,:]    # ベクトル形状なら行列形状に変換\n",
        "            \n",
        "        Z = np.exp(A) / np.sum(np.exp(A), axis=1, keepdims=True)\n",
        "        # テンソル（Z：行列）、軸（axis=1： 列の横方向に計算）\n",
        "        \n",
        "        return Z\n",
        "\n",
        "    # 以下の活性化関数を参照\n",
        "    def _logsumexp(self, X):\n",
        "        \"\"\"Calculates log(sum(exp(x))).\n",
        "        \"\"\"\n",
        "        xmax = X.max(axis=1, keepdims=True)\n",
        "        return np.log(np.exp(X - xmax).sum(axis=1, keepdims=True)) + xmax\n",
        "    \n",
        "    def _forward_propergation(self, X):\n",
        "        # 【問題2】フォワードプロパゲーションの実装\n",
        "        \n",
        "        # 1層目の計算\n",
        "        self.A1 = np.dot(X, self.W1) + self.B1\n",
        "        self.Z1 = np.tanh(self.A1)\n",
        "    \n",
        "        # 2層目の計算\n",
        "        self.A2 = np.dot(self.Z1, self.W2) + self.B2\n",
        "        self.Z2 = np.tanh(self.A2)\n",
        "        \n",
        "        # 3層目の計算\n",
        "        self.A3 = np.dot(self.Z2, self.W3) + self.B3\n",
        "        log_z3 = self.A3 - self._logsumexp(self.A3)\n",
        "        self.Z3 = np.exp(log_z3)\n",
        "        #self.Z3 = self._softmax(self.A3)\n",
        "        \n",
        "        return log_z3\n",
        "\n",
        "\n",
        "    def _loss_correct(self, X, y):\n",
        "    \n",
        "        # 【問題3】交差エントロピー誤差の実装\n",
        "        self.y = (y.reshape(-1,1) == np.arange(10))\n",
        "        log_z3 = self._forward_propergation(X)\n",
        "        L = np.sum(np.mean(-(self.y * log_z3)))\n",
        "        C = np.sum(self.Z3.argmax(axis=1) == y)\n",
        "        \n",
        "        return L, C\n",
        "\n",
        "    def _back_propergation(self, X, y, alpha=0.01):\n",
        "        # 【問題4】バックプロパゲーションの実装\n",
        "        \n",
        "        #３層目の勾配\n",
        "        LA3 = (self.Z3 - y) / self.batch_size\n",
        "        self.B3 -= alpha * np.sum(LA3, axis=0)  # B3の勾配\n",
        "        self.W3 -= alpha * np.dot(self.Z2.T, LA3) # W3の勾配\n",
        "        \n",
        "        #２層目の勾配\n",
        "        LZ2 = np.dot(LA3, self.W3.T)\n",
        "        LA2 = LZ2 * (1 - self.Z2**2)\n",
        "        self.B2 -= alpha * np.sum(LA2, axis=0)  # B2の勾配\n",
        "        self.W2 -= alpha * np.dot(self.Z1.T, LA2) # W2の勾配\n",
        "        \n",
        "        #１層目の勾配\n",
        "        LZ1 = np.dot(LA2, self.W2.T)\n",
        "        LA1 = LZ1 * (1 - self.Z1**2)\n",
        "        self.B1 -= alpha * np.sum(LA1, axis=0) # B1の勾配\n",
        "        self.W1 -= alpha * np.dot(X.T, LA1) # W1の勾配\n",
        "    \n",
        "    def fit(self, X, y, X_val=None, y_val=None):\n",
        "        \"\"\"\n",
        "        ニューラルネットワーク分類器を学習する。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (n_samples, n_features)\n",
        "            訓練データの特徴量\n",
        "        y : 次の形のndarray, shape (n_samples, )\n",
        "            訓練データの正解値\n",
        "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
        "            検証データの特徴量\n",
        "        y_val : 次の形のndarray, shape (n_samples, )\n",
        "            検証データの正解値\n",
        "        \"\"\"\n",
        "        \n",
        "        epoch = 30\n",
        "        n_step_iteration_report = 500\n",
        "        plot_data = []\n",
        "        \n",
        "        self._initalizing()\n",
        "        if self.verbose2:\n",
        "            #verboseをTrueにした際は学習過程などを出力する\n",
        "            print(\"Initialize\")\n",
        "            print(\"W1={}\".format(self.W1))\n",
        "            print(\"W2={}\".format(self.W2))\n",
        "            print(\"W3={}\".format(self.W3))\n",
        "            print(\"B1={}\".format(self.B1))\n",
        "            print(\"B2={}\".format(self.B2))\n",
        "            print(\"B3={}\".format(self.B3))\n",
        "        \n",
        "        for i in range(epoch):\n",
        "            \n",
        "            sum_of_loss = 0\n",
        "            get_mini_batch = GetMiniBatch(X, y, batch_size = self.batch_size)\n",
        "            for j, (mini_X_train, mini_y_train) in enumerate(get_mini_batch):\n",
        "                \n",
        "                # Forward Propergation & Loss\n",
        "                L,C = self._loss_correct(mini_X_train, mini_y_train)\n",
        "            \n",
        "                # Back propergation\n",
        "                self.y = mini_y_train.reshape(-1, 1) == np.arange(10)\n",
        "                self._back_propergation(mini_X_train, self.y, alpha=0.01)\n",
        "\n",
        "                # ログ出力\n",
        "                sum_of_loss += L\n",
        "                if self.verbose and (j + 1) % n_step_iteration_report == 0:\n",
        "                    train_loss = sum_of_loss / n_step_iteration_report\n",
        "                    val_loss, C = self._loss_correct(X_val, y_val)\n",
        "                    print(f'epoch: {i+1}, iteration: {j+1}, train_loss: {train_loss:.3}, val_loss: {val_loss:.3}, accuracty: {C / len(y_val):.3}')\n",
        "                    sum_of_loss = 0\n",
        "\n",
        "                    iters_per_epoch = len(X_train) / self.batch_size\n",
        "                    print(\"iters_per_epoch={}\".format(iters_per_epoch))\n",
        "                    plot_data.append((i + (j + 1) / iters_per_epoch, train_loss, val_loss))\n",
        "                \n",
        "            if self.verbose:\n",
        "                #verboseをTrueにした際は学習過程などを出力する\n",
        "                pass\n",
        "                \n",
        "                if self.verbose2:\n",
        "                    print(\"epoch={}\".format(i))\n",
        "                    print(\"forward propergation\")                    \n",
        "                    print(\" A1={}\".format(self.A1))\n",
        "                    print(\" Z1={}\".format(self.Z1))\n",
        "                    print(\" A2={}\".format(self.A2))\n",
        "                    print(\" Z2={}\".format(self.Z2))\n",
        "                    print(\" A3={}\".format(self.A3))\n",
        "                    print(\" Z3={}\".format(self.Z3))\n",
        "                    print(\"back propergation\")\n",
        "                    print(\"B3={}\".format(self.B3))                    \n",
        "                    print(\"W3={}\".format(self.W3))\n",
        "                    print(\"B2={}\".format(self.B2))\n",
        "                    print(\"W2={}\".format(self.W2))\n",
        "                    print(\"B1={}\".format(self.B1))\n",
        "                    print(\"W1={}\".format(self.W1)) \n",
        "                    \n",
        "        if self.verbose:\n",
        "            #verboseをTrueにした際は学習過程などを出力する\n",
        "            epochs, train_loss, val_loss = zip(*plot_data)\n",
        "            plt.plot(epochs, train_loss, color='r', label='train_loss')\n",
        "            plt.plot(epochs, val_loss, color='b', label='val_loss')\n",
        "            plt.xlabel('epoch')\n",
        "            plt.ylabel('loss')\n",
        "            plt.show()\n",
        "                    \n",
        "    #【問題5】推定を行うメソッド\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        ニューラルネットワーク分類器を使い推定する。\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : 次の形のndarray, shape (n_samples, n_features)\n",
        "            サンプル\n",
        "        Returns\n",
        "        -------\n",
        "            次の形のndarray, shape (n_samples, 1)\n",
        "            推定結果\n",
        "        \"\"\"\n",
        "        y_pred = self._forward_propergation(X).argmax(axis=1)\n",
        "        return y_pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjoI9JvktIgb",
        "outputId": "557d5e38-1c26-40b6-fe91-45c731eae81d"
      },
      "source": [
        "# 【問題6】学習と推定:学習\n",
        "nnc = ScratchSimpleNeuralNetrowkClassifier(seed=0, verbose = True, verbose2 = False)\n",
        "nnc.fit(X_train, y_train, X_val, y_val)\n",
        "# 【問題7】学習曲線のプロット"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1, iteration: 500, train_loss: 0.23, val_loss: 0.228, accuracty: 0.311\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 1, iteration: 1000, train_loss: 0.223, val_loss: 0.211, accuracty: 0.343\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 1, iteration: 1500, train_loss: 0.169, val_loss: 0.131, accuracty: 0.597\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 1, iteration: 2000, train_loss: 0.105, val_loss: 0.084, accuracty: 0.747\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 2, iteration: 500, train_loss: 0.0624, val_loss: 0.057, accuracty: 0.834\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 2, iteration: 1000, train_loss: 0.0538, val_loss: 0.0506, accuracty: 0.855\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 2, iteration: 1500, train_loss: 0.0503, val_loss: 0.0463, accuracty: 0.868\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 2, iteration: 2000, train_loss: 0.0451, val_loss: 0.0432, accuracty: 0.878\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 3, iteration: 500, train_loss: 0.0405, val_loss: 0.0395, accuracty: 0.888\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 3, iteration: 1000, train_loss: 0.0392, val_loss: 0.0383, accuracty: 0.894\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 3, iteration: 1500, train_loss: 0.0394, val_loss: 0.0372, accuracty: 0.899\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 3, iteration: 2000, train_loss: 0.0366, val_loss: 0.0359, accuracty: 0.901\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 4, iteration: 500, train_loss: 0.0346, val_loss: 0.0344, accuracty: 0.903\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 4, iteration: 1000, train_loss: 0.0344, val_loss: 0.034, accuracty: 0.908\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 4, iteration: 1500, train_loss: 0.0351, val_loss: 0.0333, accuracty: 0.907\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 4, iteration: 2000, train_loss: 0.0329, val_loss: 0.0325, accuracty: 0.911\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 5, iteration: 500, train_loss: 0.0313, val_loss: 0.0316, accuracty: 0.911\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 5, iteration: 1000, train_loss: 0.0316, val_loss: 0.0313, accuracty: 0.914\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 5, iteration: 1500, train_loss: 0.0322, val_loss: 0.0307, accuracty: 0.914\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 5, iteration: 2000, train_loss: 0.0302, val_loss: 0.03, accuracty: 0.918\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 6, iteration: 500, train_loss: 0.0288, val_loss: 0.0293, accuracty: 0.918\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 6, iteration: 1000, train_loss: 0.0292, val_loss: 0.0291, accuracty: 0.92\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 6, iteration: 1500, train_loss: 0.0298, val_loss: 0.0285, accuracty: 0.921\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 6, iteration: 2000, train_loss: 0.0279, val_loss: 0.0279, accuracty: 0.924\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 7, iteration: 500, train_loss: 0.0265, val_loss: 0.0273, accuracty: 0.923\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 7, iteration: 1000, train_loss: 0.027, val_loss: 0.0272, accuracty: 0.925\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 7, iteration: 1500, train_loss: 0.0276, val_loss: 0.0266, accuracty: 0.926\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 7, iteration: 2000, train_loss: 0.0259, val_loss: 0.026, accuracty: 0.927\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 8, iteration: 500, train_loss: 0.0246, val_loss: 0.0256, accuracty: 0.927\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 8, iteration: 1000, train_loss: 0.0251, val_loss: 0.0255, accuracty: 0.93\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 8, iteration: 1500, train_loss: 0.0257, val_loss: 0.0249, accuracty: 0.931\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 8, iteration: 2000, train_loss: 0.0241, val_loss: 0.0244, accuracty: 0.931\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 9, iteration: 500, train_loss: 0.0228, val_loss: 0.024, accuracty: 0.93\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 9, iteration: 1000, train_loss: 0.0233, val_loss: 0.024, accuracty: 0.933\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 9, iteration: 1500, train_loss: 0.024, val_loss: 0.0233, accuracty: 0.935\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 9, iteration: 2000, train_loss: 0.0225, val_loss: 0.0229, accuracty: 0.936\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 10, iteration: 500, train_loss: 0.0211, val_loss: 0.0225, accuracty: 0.935\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 10, iteration: 1000, train_loss: 0.0217, val_loss: 0.0226, accuracty: 0.937\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 10, iteration: 1500, train_loss: 0.0223, val_loss: 0.022, accuracty: 0.938\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 10, iteration: 2000, train_loss: 0.0209, val_loss: 0.0217, accuracty: 0.939\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 11, iteration: 500, train_loss: 0.0196, val_loss: 0.0213, accuracty: 0.939\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 11, iteration: 1000, train_loss: 0.0202, val_loss: 0.0214, accuracty: 0.94\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 11, iteration: 1500, train_loss: 0.0208, val_loss: 0.0208, accuracty: 0.942\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 11, iteration: 2000, train_loss: 0.0195, val_loss: 0.0205, accuracty: 0.943\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 12, iteration: 500, train_loss: 0.0182, val_loss: 0.0201, accuracty: 0.942\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 12, iteration: 1000, train_loss: 0.0188, val_loss: 0.0203, accuracty: 0.943\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 12, iteration: 1500, train_loss: 0.0195, val_loss: 0.0197, accuracty: 0.944\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 12, iteration: 2000, train_loss: 0.0182, val_loss: 0.0195, accuracty: 0.945\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 13, iteration: 500, train_loss: 0.0169, val_loss: 0.0191, accuracty: 0.945\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 13, iteration: 1000, train_loss: 0.0176, val_loss: 0.0193, accuracty: 0.945\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 13, iteration: 1500, train_loss: 0.0183, val_loss: 0.0187, accuracty: 0.948\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 13, iteration: 2000, train_loss: 0.017, val_loss: 0.0186, accuracty: 0.947\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 14, iteration: 500, train_loss: 0.0157, val_loss: 0.0182, accuracty: 0.948\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 14, iteration: 1000, train_loss: 0.0166, val_loss: 0.0183, accuracty: 0.948\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 14, iteration: 1500, train_loss: 0.0172, val_loss: 0.0178, accuracty: 0.95\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 14, iteration: 2000, train_loss: 0.0159, val_loss: 0.0177, accuracty: 0.95\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 15, iteration: 500, train_loss: 0.0147, val_loss: 0.0174, accuracty: 0.95\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 15, iteration: 1000, train_loss: 0.0156, val_loss: 0.0175, accuracty: 0.949\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 15, iteration: 1500, train_loss: 0.0162, val_loss: 0.017, accuracty: 0.953\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 15, iteration: 2000, train_loss: 0.0149, val_loss: 0.017, accuracty: 0.953\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 16, iteration: 500, train_loss: 0.0137, val_loss: 0.0167, accuracty: 0.953\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 16, iteration: 1000, train_loss: 0.0147, val_loss: 0.0168, accuracty: 0.951\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 16, iteration: 1500, train_loss: 0.0153, val_loss: 0.0163, accuracty: 0.954\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 16, iteration: 2000, train_loss: 0.014, val_loss: 0.0163, accuracty: 0.954\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 17, iteration: 500, train_loss: 0.0129, val_loss: 0.016, accuracty: 0.955\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 17, iteration: 1000, train_loss: 0.0139, val_loss: 0.0161, accuracty: 0.953\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 17, iteration: 1500, train_loss: 0.0144, val_loss: 0.0157, accuracty: 0.956\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 17, iteration: 2000, train_loss: 0.0132, val_loss: 0.0157, accuracty: 0.956\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 18, iteration: 500, train_loss: 0.0121, val_loss: 0.0154, accuracty: 0.956\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 18, iteration: 1000, train_loss: 0.0132, val_loss: 0.0154, accuracty: 0.955\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 18, iteration: 1500, train_loss: 0.0137, val_loss: 0.0151, accuracty: 0.957\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 18, iteration: 2000, train_loss: 0.0124, val_loss: 0.0151, accuracty: 0.957\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 19, iteration: 500, train_loss: 0.0114, val_loss: 0.0148, accuracty: 0.958\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 19, iteration: 1000, train_loss: 0.0126, val_loss: 0.0149, accuracty: 0.957\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 19, iteration: 1500, train_loss: 0.013, val_loss: 0.0146, accuracty: 0.958\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 19, iteration: 2000, train_loss: 0.0117, val_loss: 0.0146, accuracty: 0.958\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 20, iteration: 500, train_loss: 0.0108, val_loss: 0.0143, accuracty: 0.959\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 20, iteration: 1000, train_loss: 0.012, val_loss: 0.0144, accuracty: 0.959\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 20, iteration: 1500, train_loss: 0.0123, val_loss: 0.0141, accuracty: 0.959\n",
            "iters_per_epoch=2400.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 20, iteration: 2000, train_loss: 0.011, val_loss: 0.0142, accuracty: 0.959\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 21, iteration: 500, train_loss: 0.0102, val_loss: 0.0139, accuracty: 0.96\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 21, iteration: 1000, train_loss: 0.0114, val_loss: 0.0139, accuracty: 0.96\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 21, iteration: 1500, train_loss: 0.0117, val_loss: 0.0136, accuracty: 0.96\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 21, iteration: 2000, train_loss: 0.0104, val_loss: 0.0137, accuracty: 0.96\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 22, iteration: 500, train_loss: 0.0096, val_loss: 0.0135, accuracty: 0.962\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 22, iteration: 1000, train_loss: 0.0108, val_loss: 0.0135, accuracty: 0.961\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 22, iteration: 1500, train_loss: 0.0111, val_loss: 0.0133, accuracty: 0.961\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 22, iteration: 2000, train_loss: 0.00982, val_loss: 0.0133, accuracty: 0.961\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 23, iteration: 500, train_loss: 0.00909, val_loss: 0.0131, accuracty: 0.963\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 23, iteration: 1000, train_loss: 0.0103, val_loss: 0.0131, accuracty: 0.962\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 23, iteration: 1500, train_loss: 0.0106, val_loss: 0.0129, accuracty: 0.963\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 23, iteration: 2000, train_loss: 0.00931, val_loss: 0.013, accuracty: 0.963\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 24, iteration: 500, train_loss: 0.00862, val_loss: 0.0128, accuracty: 0.964\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 24, iteration: 1000, train_loss: 0.00985, val_loss: 0.0127, accuracty: 0.963\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 24, iteration: 1500, train_loss: 0.0101, val_loss: 0.0126, accuracty: 0.964\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 24, iteration: 2000, train_loss: 0.00883, val_loss: 0.0127, accuracty: 0.964\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 25, iteration: 500, train_loss: 0.00817, val_loss: 0.0124, accuracty: 0.965\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 25, iteration: 1000, train_loss: 0.00939, val_loss: 0.0124, accuracty: 0.963\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 25, iteration: 1500, train_loss: 0.00965, val_loss: 0.0123, accuracty: 0.964\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 25, iteration: 2000, train_loss: 0.00839, val_loss: 0.0124, accuracty: 0.965\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 26, iteration: 500, train_loss: 0.00776, val_loss: 0.0122, accuracty: 0.965\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 26, iteration: 1000, train_loss: 0.00897, val_loss: 0.0121, accuracty: 0.964\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 26, iteration: 1500, train_loss: 0.00921, val_loss: 0.012, accuracty: 0.965\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 26, iteration: 2000, train_loss: 0.00798, val_loss: 0.0121, accuracty: 0.965\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 27, iteration: 500, train_loss: 0.00737, val_loss: 0.0119, accuracty: 0.966\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 27, iteration: 1000, train_loss: 0.00856, val_loss: 0.0118, accuracty: 0.965\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 27, iteration: 1500, train_loss: 0.00879, val_loss: 0.0117, accuracty: 0.966\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 27, iteration: 2000, train_loss: 0.00759, val_loss: 0.0118, accuracty: 0.966\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 28, iteration: 500, train_loss: 0.007, val_loss: 0.0117, accuracty: 0.966\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 28, iteration: 1000, train_loss: 0.00817, val_loss: 0.0116, accuracty: 0.966\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 28, iteration: 1500, train_loss: 0.0084, val_loss: 0.0115, accuracty: 0.966\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 28, iteration: 2000, train_loss: 0.00724, val_loss: 0.0116, accuracty: 0.966\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 29, iteration: 500, train_loss: 0.00665, val_loss: 0.0114, accuracty: 0.967\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 29, iteration: 1000, train_loss: 0.00781, val_loss: 0.0113, accuracty: 0.967\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 29, iteration: 1500, train_loss: 0.00802, val_loss: 0.0113, accuracty: 0.967\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 29, iteration: 2000, train_loss: 0.0069, val_loss: 0.0114, accuracty: 0.967\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 30, iteration: 500, train_loss: 0.00633, val_loss: 0.0112, accuracty: 0.967\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 30, iteration: 1000, train_loss: 0.00746, val_loss: 0.0111, accuracty: 0.968\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 30, iteration: 1500, train_loss: 0.00767, val_loss: 0.0111, accuracty: 0.967\n",
            "iters_per_epoch=2400.0\n",
            "epoch: 30, iteration: 2000, train_loss: 0.00658, val_loss: 0.0112, accuracty: 0.967\n",
            "iters_per_epoch=2400.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gYWhwoltIgf"
      },
      "source": [
        "# 【問題6】学習と推定:推定\n",
        "y_pred = nnc.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkYq92MhtIgg",
        "outputId": "e1b1257f-95d0-4f30-a5ae-9229a01c544a"
      },
      "source": [
        "y_pred"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, ..., 4, 5, 6], dtype=int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVJbtpIstIgh",
        "outputId": "fef25984-0524-465b-e80e-42a0e7338a71"
      },
      "source": [
        "# 【問題8】（アドバンス課題）誤分類の確認\n",
        "\"\"\"\n",
        "語分類結果を並べて表示する。画像の上の表示は「推定結果/正解」である。\n",
        "Parameters:\n",
        "----------\n",
        "y_pred : 推定値のndarray (n_samples,)\n",
        "y_val : 検証データの正解ラベル(n_samples,)\n",
        "X_val : 検証データの特徴量（n_samples, n_features)\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "num = 36 # いくつ表示するか\n",
        "\n",
        "true_false = y_pred==y_val\n",
        "false_list = np.where(true_false==False)[0].astype(np.int)\n",
        "if false_list.shape[0] < num:\n",
        "    num = false_list.shape[0]\n",
        "fig = plt.figure(figsize=(6, 6))\n",
        "fig.subplots_adjust(left=0, right=0.8,  bottom=0, top=0.8, hspace=1, wspace=0.5)\n",
        "for i in range(num):\n",
        "    ax = fig.add_subplot(6, 6, i + 1, xticks=[], yticks=[])\n",
        "    ax.set_title(\"{} / {}\".format(y_pred[false_list[i]],y_val[false_list[i]]))\n",
        "    ax.imshow(X_val.reshape(-1,28,28)[false_list[i]], cmap='gray')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<ipython-input-59-2c68b602cd2f>:14: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "  true_false = y_pred==y_val\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAADEAAABBCAYAAAB4vkzdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEgElEQVR4nO2Z30scVxTHP9d1V7SrO5DsJtBuRQlIjKwERNOK282Lhda3IipC4kOo+D8U2se8t2C3YGkreTKvbUwwQivsQ3wQZAlBNP5cRANJjD/25+zpg0Fo6u7MuLPrttkv3JeZc885n7ln7r1zR4kI/3VVnXcCdqgCUS6qQJSLKhDlovcbQil18E7TlVLfG/T5SSn19SnXf3zHV1IptW86GREpuAEfAAdA0MBuA/jIhL9fgJ9Nx7cJ4jbwHFB5bALAoskHsg98Zja+Xe/EbeA3yb8R+wL43YSvr4AXwF+mo9swCh8DOtBkYDcH9Jjw9xj4zlIONkB8A/xpYKO9fboOAzs/kAGareRgRzndAn41sPkceCwiuglfERF5biWBgiCUUp8CHwJTBqZfAn+YcHmL45nJmgospTAwaWCjgG3AZ2D3CXAI1FvNQ+WfUAqXUqoT+EFEOosVo1Tbjm+L6bzoI1EKvd8bwHJStRVjpdS51p6IqNOu/y9GogJRLqpAlIsqEOWikkKEQiHC4TBLS0vs7u6yu7vL5uYms7OzDA0Nndmvpb2TmcWutbWV4eFhLl++/K97bW1tNDU1oWka1dXH62w2myUej7O2tkYgECBfPrkWO1sh+vv76e3tJRgM4vF43u1LQ0MDLpfr1L5HR0d4PB6y2WxO/7kgLG07ckkphc/n486dO1y/fh1N03A4HCf3dV1nfX2dly9fsrW1RSwWI5lMcuHCBVpaWmhtbS0ovi0QTqeTzs5OQqHQSZmk02nS6TRHR0dsbGwwPT1NMplkfn6e+fl5Dg8PuXHjBmNjY1y9epV0On2+EC6Xi97eXqqrq0kkEui6TiwWY3t7m2fPnjExMcHi4uI/Eq2pqaG5uZlgMEg2m2VzczPv+1B0CDj+Vtd1nXv37rG6usqDBw+IRqNkMplT7R0OB5qmcenSJV6/fs3du3fPDGH1YEBytbq6OtE0TVwul7ydAPK2kZERiUQisrCwIO3t7eJyuQz7FPPcCTieXfb29kilUoZP9MqVK3R3d9PS0kI2m+Xg4IBUKnXm2LYudmbLIRQKEQgESKVSPH36lL29vYLilnzb4fV66enpwe/3s7W1RSQS4c2bNwX5LDlEfX09fr+furo6lpeXmZubK6iU4BwgnE4nVVVVZDIZYrEY0Wi0YJ8lhVBK0djYiNvtBsi7xbAi29YJM/J6vYyPj+P3+1lbW2NnZ8cWvyUbCaUUo6Oj+Hw+AB4+fMjMzIw9zu1a7IyaUkru378v8XhcwuGwdHR0iMPhsOQjV14lKaeamhr6+vro6uri1atXPHr0iJWVFXTd6J+LSRV7JNxut9y8eVNmZ2clkUjI1NSUXLt2zfIokGckigrhdDqlra1NJicnJZlMyv7+vgwODoqmaWcqyXOB8Hg8MjAwILqui67r8uTJkzMD5IMo6uzkdrtPZqN0Ok04HCYej9sep6gQtbW1XLx4kUwmQzQaZWZmpqAvuFyyelDwAli3PQtzahQR72k3Kr+7ykUViHJRBaJcVIEoF1UgykV/Ay85kfu/AxSwAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEeVbT8wtIgh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}