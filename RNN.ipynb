{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "i5g4W2FhNQjg",
        "m8G1H1ykNQjk",
        "-AkN4t5TNQjl"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/libra3910/diveintocode-ml/blob/master/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_6f9OL6NQjU"
      },
      "source": [
        "### IMDBの取り込み"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rb1aWdFaNQjX"
      },
      "source": [
        "# IMDBをカレントフォルダにダウンロード\n",
        "#!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "# 解凍\n",
        "#!tar zxf aclImdb_v1.tar.gz\n",
        "# aclImdb/train/unsupはラベル無しのため削除\n",
        "#!rm -rf aclImdb/train/unsup\n",
        "# IMDBデータセットの説明を表示\n",
        "#!cat aclImdb/README"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vpv0MPhHNQjY",
        "outputId": "dc523b2b-77d9-4d29-fa63-6c9a45e3f84d"
      },
      "source": [
        "from sklearn.datasets import load_files\n",
        "train_review = load_files('./aclImdb_v1/aclImdb/train/', encoding='utf-8')\n",
        "x_train, y_train = train_review.data, train_review.target\n",
        "test_review = load_files('./aclImdb_v1/aclImdb/test/', encoding='utf-8')\n",
        "x_test, y_test = test_review.data, test_review.target\n",
        "# ラベルの0,1と意味の対応の表示\n",
        "print(train_review.target_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['neg', 'pos', 'unsup']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMEU4vAfNQjZ"
      },
      "source": [
        "### 【問題1】BoWのスクラッチ実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elqo05XENQja",
        "outputId": "b837082e-3c56-46e5-f59f-19fc1d8d602d"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# フレーズの代入\n",
        "phrase = []\n",
        "phrase.append('This movie is SOOOO funny!!!')\n",
        "phrase.append('What a movie! I never')\n",
        "phrase.append('best movie ever!!!!! this movie')\n",
        "\n",
        "# BOW:1-gram\n",
        "bow1 =[]\n",
        "all_bows = []\n",
        "for i in range(len(phrase)):\n",
        "    bow1.append(phrase[i].split(' '))\n",
        "all_bows = np.array(bow1).reshape(-1)\n",
        "all_bows = np.unique(all_bows)\n",
        "\n",
        "all_bows_count = []\n",
        "for j in range(all_bows.shape[0]):\n",
        "    all_bows_count.append((all_bows[j], bow1[0].count(all_bows[j]), bow1[1].count(all_bows[j]), bow1[2].count(all_bows[j])))\n",
        "\n",
        "print(\"BOW:1-gram\")\n",
        "print(\"１．元のフレーズを表示：\")\n",
        "print(phrase)\n",
        "print(\"２．フレーズを分割：\")\n",
        "print(bow1)\n",
        "print(\"３．単語の列を作成：\")\n",
        "print(all_bows)\n",
        "print(\"４．フレーズごとの単語の出現回数：\")\n",
        "print(pd.DataFrame(np.array(all_bows_count).reshape(-1,4).T))\n",
        "\n",
        "# BOW:2-gram\n",
        "bow2 = []\n",
        "for j in range(len(bow1)):\n",
        "    tmp = []\n",
        "    for k in range(len(bow1[j])-1):\n",
        "        tmp.append(bow1[j][k] + ' ' + bow1[j][k+1])\n",
        "    bow2.append(tmp)\n",
        "all_bows2 = np.array(bow2).reshape(-1)\n",
        "all_bows2 = np.unique(all_bows2)\n",
        "\n",
        "all_bows_count2 = []\n",
        "for j in range(all_bows2.shape[0]):\n",
        "    all_bows_count2.append((all_bows2[j], bow2[0].count(all_bows2[j]), bow2[1].count(all_bows2[j]), bow2[2].count(all_bows2[j])))\n",
        "\n",
        "print()\n",
        "print(\"BOW:2-gram\")\n",
        "print(\"１．元のフレーズを表示：\")\n",
        "print(phrase)\n",
        "print(\"２．フレーズを分割：\")\n",
        "print(bow2)\n",
        "print(\"３．単語の列を作成：\")\n",
        "print(all_bows2)\n",
        "print(\"４．フレーズごとの単語の出現回数：\")\n",
        "print(pd.DataFrame(np.array(all_bows_count2).reshape(-1,4).T))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BOW:1-gram\n",
            "１．元のフレーズを表示：\n",
            "['This movie is SOOOO funny!!!', 'What a movie! I never', 'best movie ever!!!!! this movie']\n",
            "２．フレーズを分割：\n",
            "[['This', 'movie', 'is', 'SOOOO', 'funny!!!'], ['What', 'a', 'movie!', 'I', 'never'], ['best', 'movie', 'ever!!!!!', 'this', 'movie']]\n",
            "３．単語の列を作成：\n",
            "['I' 'SOOOO' 'This' 'What' 'a' 'best' 'ever!!!!!' 'funny!!!' 'is' 'movie'\n",
            " 'movie!' 'never' 'this']\n",
            "４．フレーズごとの単語の出現回数：\n",
            "  0      1     2     3  4     5          6         7   8      9       10  \\\n",
            "0  I  SOOOO  This  What  a  best  ever!!!!!  funny!!!  is  movie  movie!   \n",
            "1  0      1     1     0  0     0          0         1   1      1       0   \n",
            "2  1      0     0     1  1     0          0         0   0      0       1   \n",
            "3  0      0     0     0  0     1          1         0   0      2       0   \n",
            "\n",
            "      11    12  \n",
            "0  never  this  \n",
            "1      0     0  \n",
            "2      1     0  \n",
            "3      0     1  \n",
            "\n",
            "BOW:2-gram\n",
            "１．元のフレーズを表示：\n",
            "['This movie is SOOOO funny!!!', 'What a movie! I never', 'best movie ever!!!!! this movie']\n",
            "２．フレーズを分割：\n",
            "[['This movie', 'movie is', 'is SOOOO', 'SOOOO funny!!!'], ['What a', 'a movie!', 'movie! I', 'I never'], ['best movie', 'movie ever!!!!!', 'ever!!!!! this', 'this movie']]\n",
            "３．単語の列を作成：\n",
            "['I never' 'SOOOO funny!!!' 'This movie' 'What a' 'a movie!' 'best movie'\n",
            " 'ever!!!!! this' 'is SOOOO' 'movie ever!!!!!' 'movie is' 'movie! I'\n",
            " 'this movie']\n",
            "４．フレーズごとの単語の出現回数：\n",
            "        0               1           2       3         4           5   \\\n",
            "0  I never  SOOOO funny!!!  This movie  What a  a movie!  best movie   \n",
            "1        0               1           1       0         0           0   \n",
            "2        1               0           0       1         1           0   \n",
            "3        0               0           0       0         0           1   \n",
            "\n",
            "               6         7                8         9         10          11  \n",
            "0  ever!!!!! this  is SOOOO  movie ever!!!!!  movie is  movie! I  this movie  \n",
            "1               0         1                0         1         0           0  \n",
            "2               0         0                0         0         1           0  \n",
            "3               1         0                1         0         0           1  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAz0DEf7NQjb"
      },
      "source": [
        "### 【問題2】TF-IDFの計算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3asIZCXdNQjb",
        "outputId": "ff470392-b12a-4024-e64f-ff2374b48982"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Full of (then) unknown actors TSF is a great big cuddly romp of a film.<br /><br />The idea of a bunch of bored teenagers ripping off the local sink factory is odd enough, but add in the black humour that Forsyth & Co are so good at and your in for a real treat.<br /><br />The comatose van driver by itself worth seeing, and the canal side chase is just too real to be anything but funny.<br /><br />And for anyone who lived in Glasgow it\\'s a great \"Oh I know where that is\" film.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwD0pXUWNQjc",
        "outputId": "1364b6f0-e5fa-449a-802b-d01dce63f470"
      },
      "source": [
        "# 後で、この出力結果を問題４にも使用する\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = []\n",
        "s_cnt = 0\n",
        "i = 0\n",
        "while s_cnt < 5000:\n",
        "    corpus.append(x_train[i])\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(corpus) # １.corpusが元のフレーズ\n",
        "    s_cnt += len(vectorizer.get_feature_names()) # ２.フレーズを分割、３.単語の列を作成\n",
        "    i = i + 1\n",
        "print(s_cnt)\n",
        "print(i)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5252\n",
            "10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MHswKyNNQjc",
        "outputId": "e37e7208-acac-4f06-8fa8-cf96931242ca"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# ３．単語の列を作成\n",
        "\n",
        "sample = []\n",
        "# ベクトル化する文字列\n",
        "for i in range(9):\n",
        "    sample.append(x_train[i])\n",
        "sample = np.array(sample)\n",
        "\n",
        "# TfidfVectorizer\n",
        "vec_tfidf = TfidfVectorizer()\n",
        "\n",
        "# ベクトル化\n",
        "X = vec_tfidf.fit_transform(sample)\n",
        "\n",
        "print('Vocabulary size: {}'.format(len(vec_tfidf.vocabulary_)))\n",
        "print('Vocabulary content: {}'.format(vec_tfidf.vocabulary_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocabulary size: 859\n",
            "Vocabulary content: {'full': 288, 'of': 514, 'then': 732, 'unknown': 787, 'actors': 15, 'tsf': 774, 'is': 386, 'great': 312, 'big': 80, 'cuddly': 169, 'romp': 613, 'film': 270, 'br': 96, 'the': 729, 'idea': 359, 'bunch': 104, 'bored': 89, 'teenagers': 721, 'ripping': 602, 'off': 515, 'local': 442, 'sink': 666, 'factory': 250, 'odd': 513, 'enough': 234, 'but': 106, 'add': 19, 'in': 368, 'black': 84, 'humour': 356, 'that': 727, 'forsyth': 283, 'co': 133, 'are': 47, 'so': 670, 'good': 307, 'at': 51, 'and': 35, 'your': 858, 'for': 277, 'real': 579, 'treat': 768, 'comatose': 137, 'van': 798, 'driver': 213, 'by': 107, 'itself': 396, 'worth': 847, 'seeing': 637, 'canal': 112, 'side': 658, 'chase': 126, 'just': 405, 'too': 760, 'to': 754, 'be': 61, 'anything': 43, 'funny': 289, 'anyone': 42, 'who': 831, 'lived': 440, 'glasgow': 302, 'it': 392, 'oh': 516, 'know': 417, 'where': 827, 'amount': 32, 'disappointment': 201, 'am': 27, 'getting': 297, 'these': 734, 'days': 174, 'movies': 488, 'like': 435, 'partner': 532, 'jhoom': 400, 'barabar': 57, 'now': 511, 'heyy': 341, 'babyy': 54, 'gonna': 306, 'end': 230, 'my': 497, 'habit': 320, 'first': 274, 'day': 173, 'shows': 656, 'movie': 487, 'an': 33, 'utter': 797, 'because': 64, 'had': 321, 'potential': 555, 'become': 65, 'laugh': 423, 'riot': 601, 'only': 521, 'if': 361, 'débutant': 215, 'director': 200, 'sajid': 620, 'khan': 412, 'hadn': 322, 'tried': 770, 'many': 462, 'things': 736, 'saving': 625, 'grace': 310, 'were': 824, 'last': 422, 'thirty': 739, 'minutes': 475, 'which': 828, 'seriously': 644, 'elsewhere': 227, 'fails': 252, 'miserably': 476, 'half': 323, 'was': 812, 'desperately': 187, 'been': 66, 'look': 445, 'wasn': 814, 'next': 505, '45': 8, 'emotional': 229, 'looked': 446, 'totally': 763, 'artificial': 49, 'illogical': 363, 'ok': 518, 'when': 826, 'you': 856, 'out': 527, 'this': 740, 'don': 207, 'expect': 247, 'much': 493, 'logic': 443, 'all': 24, 'flaws': 276, 'tend': 723, 'appear': 45, 'enjoy': 233, 'thats': 728, 'case': 117, 'with': 837, 'acting': 14, 'not': 509, 'keep': 409, 'one': 520, 'interested': 378, 'positives': 553, 'can': 111, 'take': 714, 'hot': 352, 'actresses': 17, '30': 7, 'some': 672, 'comic': 139, 'scenes': 628, 'lead': 424, 'cast': 118, 'baby': 53, 'problem': 563, 'do': 204, 'come': 138, 'together': 757, 'properly': 566, 'make': 457, 'anyways': 44, 'read': 578, 'somewhere': 675, 'isn': 387, 'copy': 154, 'three': 744, 'men': 471, 'think': 737, 'would': 849, 'have': 332, 'better': 78, 'future': 290, 'we': 820, 'told': 758, 'what': 825, 'philip': 544, 'dick': 192, 'did': 193, 'want': 810, 'chance': 123, 'he': 333, 'wrote': 853, 'short': 654, 'stories': 692, 'about': 11, 'man': 460, 'as': 50, 'society': 671, 'headed': 334, 'blade': 85, 'runner': 615, 'total': 762, 'recall': 586, 'paycheck': 537, 'screamers': 633, 'minority': 474, 'report': 593, 'written': 851, 'turned': 776, 'into': 382, 'most': 483, 'less': 430, 'than': 726, 'enthusiastic': 235, 'view': 803, 'see': 636, 'effects': 222, 'predicting': 559, 'point': 551, 'crimes': 164, 'prevented': 561, 'arresting': 48, 'murderers': 495, 'before': 67, 'they': 735, 'kill': 413, 'does': 205, 'logical': 444, 'there': 733, 'quick': 573, 'little': 438, 'scene': 627, 'early': 217, 'addresses': 20, 'those': 741, 'concerns': 148, 'on': 519, 'surface': 708, 'makes': 458, 'sense': 640, 'tom': 759, 'cruise': 168, 'plays': 548, 'washington': 813, 'dc': 175, 'pre': 557, 'crime': 163, 'chief': 127, 'john': 402, 'anderton': 37, 'runs': 616, 'investigators': 384, 'rely': 592, 'scientifically': 631, 'engineered': 232, 'beings': 70, 'murders': 496, 'happen': 325, 'system': 713, 'course': 159, 'raises': 575, 'civil': 130, 'liberty': 431, 'issues': 391, 'seems': 638, 'work': 842, 'perfectly': 540, 'until': 791, 'fingered': 273, 'murder': 494, 'rest': 595, 'tries': 771, 'prove': 567, 'innocent': 374, 'also': 25, 'set': 646, 'up': 792, 'possibly': 554, 'oily': 517, 'department': 180, 'justice': 406, 'figure': 269, 'investigating': 383, 'precrime': 558, 'goes': 305, 'national': 501, 'after': 21, 'election': 224, 'played': 547, 'colin': 136, 'farrell': 258, 'directed': 199, 'steven': 687, 'spielberg': 679, 'both': 93, 'whodunnit': 832, 'futuristic': 291, 'exercise': 246, 'science': 630, 'fiction': 266, 'time': 749, 'spent': 678, 'designing': 184, '2050s': 6, 'including': 370, 'cars': 116, 'run': 614, 'magnets': 456, 'virtual': 805, 'reality': 582, 'stations': 686, 'more': 482, 'throughout': 747, 'interesting': 379, 'design': 183, 'sick': 657, 'sticks': 688, 'used': 795, 'cops': 153, 'bring': 99, 'down': 209, 'criminals': 165, 'blueish': 86, 'tint': 751, 'given': 300, 'gives': 301, 'us': 794, 'cold': 135, 'feeling': 261, 'loving': 453, 'or': 522, 'hospitable': 351, 'live': 439, 'another': 40, 'trait': 767, 'story': 693, 'wonderful': 841, 'works': 844, 'buff': 103, 'fan': 257, 'dan': 172, 'katzir': 408, 'has': 330, 'produced': 564, 'takes': 715, 'roller': 607, 'coaster': 134, 'ride': 599, 'through': 746, 'romance': 609, 'troubles': 772, 'surrounding': 710, 'modern': 478, 'israel': 389, 'ever': 241, 'love': 452, 'brings': 100, 'back': 55, 'uncertainties': 780, 'insecurities': 375, 'heartache': 335, 'bitter': 82, 'sweet': 712, 'atmosphere': 52, 'fear': 260, 'isolation': 388, 'came': 109, 'difficult': 196, 'times': 750, 'serve': 645, 'intensify': 377, 'instantly': 376, 'drawn': 211, 'plight': 549, 'fail': 251, 'deeply': 179, 'moved': 486, 'write': 850, 'drama': 210, 'passion': 534, 'contrast': 151, 'between': 79, 'realities': 581, 'desperate': 186, 'snatched': 669, 'relationship': 590, 'iris': 385, 'state': 685, 'turmoil': 775, 'eminently': 228, 'watchable': 818, 'ounce': 525, 'scream': 632, 'studio': 695, 'horror': 350, 'product': 565, 'get': 296, 'forced': 278, 'bother': 94, 'well': 822, 'kept': 411, 'me': 469, 'thinking': 738, 'say': 626, 'importance': 365, 'myth': 499, 'our': 526, 'lives': 441, 'how': 353, 'children': 128, 'interpret': 380, 'world': 845, 'violence': 804, 'ransacking': 577, 'environment': 236, 'ignorance': 362, 'its': 395, 'history': 344, 'legends': 428, 'here': 339, 'flatly': 275, 'could': 156, 'technically': 720, 'call': 108, 'monster': 481, 'even': 240, 'though': 742, 'wendigo': 823, 'physical': 545, 'form': 282, 'beliefs': 71, 'happening': 326, 'legendary': 427, 'spirit': 680, 'beast': 62, 'standard': 681, 'thriller': 745, 'elements': 225, 'looking': 447, 'basics': 60, 'never': 503, 'bores': 90, 'fact': 249, 'creature': 161, 'fessenden': 264, 'successfully': 698, 'continues': 150, 'george': 295, 'romero': 612, 'tradition': 765, 'using': 796, 'genre': 294, 'parable': 530, 'discussion': 202, 'forum': 284, 'while': 829, 'still': 689, 'keeping': 410, 'creeped': 162, 'although': 26, 'mold': 479, '1949': 3, 'appears': 46, 'somewhat': 674, 'melodramatic': 470, 'today': 756, 'white': 830, 'riso': 603, 'amaro': 28, 'italian': 393, 'rice': 598, 'surely': 707, 'ranks': 576, 'among': 31, 'classics': 131, 'very': 802, 'guiseppe': 317, 'de': 176, 'santis': 622, 'pretty': 560, 'ordinary': 523, 'excellently': 245, 'interwoven': 381, 'impressive': 367, 'decor': 178, 'harsh': 329, 'season': 634, 'labor': 420, 'fields': 268, 'northern': 508, 'italy': 394, 'thousands': 743, 'women': 839, 'their': 730, 'ankles': 39, 'water': 819, 'breaking': 97, 'backs': 56, 'burning': 105, 'sun': 704, 'earn': 218, 'few': 265, 'bucks': 101, 'truly': 773, 'setting': 647, 'labeled': 419, 'neo': 502, 'realism': 580, 'issue': 390, 'mentioning': 472, 'female': 263, 'silvana': 659, 'mangano': 461, 'ex': 244, 'miss': 477, 'rome': 611, 'standards': 682, 'performance': 541, 'shocking': 653, 'earned': 219, 'lot': 451, 'publicity': 569, 'particular': 531, 'strongly': 694, 'roman': 608, 'catholic': 119, 'gloomy': 303, 'sunday': 705, 'ein': 223, 'lied': 433, 'von': 808, 'liebe': 432, 'und': 781, 'tod': 755, 'rolf': 606, 'schübel': 629, '1999': 5, 'romantic': 610, 'absorbing': 12, 'beautiful': 63, 'heartbreaking': 336, 'started': 684, 'jules': 403, 'jim': 401, 'ended': 231, 'agatha': 23, 'christie': 129, 'books': 88, 'said': 619, 'something': 673, 'friendship': 286, 'devotion': 190, 'jealousy': 398, 'war': 811, 'holocaust': 348, 'dignity': 198, 'betrayal': 76, 'book': 87, 'popular': 552, 'perfect': 539, 'made': 455, 'cynic': 171, 'wonder': 840, 'complexity': 143, 'relationships': 591, 'sensational': 639, 'revelations': 596, 'whom': 833, 'simply': 660, 'overwhelmed': 529, 'unforgettable': 784, 'four': 285, 'parts': 533, 'tragic': 766, 'triangle': 769, 'rectangle': 588, 'terrific': 724, 'believe': 72, 'fell': 262, 'girl': 298, 'dignified': 197, 'ilona': 364, 'star': 683, 'making': 459, 'young': 857, 'hungarian': 357, 'actress': 16, 'erica': 237, 'marozsán': 463, 'titular': 753, 'song': 676, 'haunting': 331, 'sad': 618, 'no': 506, 'doubt': 208, 'deserves': 182, 'effect': 221, 'countless': 157, 'listeners': 437, 'surprised': 709, 'known': 418, 'country': 158, 'gem': 293, 'based': 59, 'such': 699, 'important': 366, 'role': 605, 'characters': 125, 'research': 594, 'behind': 68, 'death': 177, 'fascinating': 259, 'fictional': 267, 'composed': 144, '1930s': 1, 'rezsö': 597, 'seress': 643, 'believed': 73, 'caused': 121, 'suicides': 703, 'hungary': 358, 'over': 528, 'europe': 239, 'moving': 489, 'toward': 764, 'devastating': 188, 'century': 122, 'jewish': 399, 'pianist': 546, 'composer': 145, 'thrown': 748, 'concentration': 147, 'camp': 110, 'survived': 711, 'unlike': 788, 'his': 343, 'mother': 485, 'january': 397, '1968': 4, 'committed': 140, 'suicide': 702, 'budapest': 102, 'jumping': 404, 'window': 836, 'according': 13, 'obituary': 512, 'new': 504, 'york': 855, 'mr': 490, 'seres': 642, 'complained': 141, 'success': 697, 'actually': 18, 'increased': 372, 'unhappiness': 785, 'knew': 416, 'able': 10, 'second': 635, 'hit': 345, 'singers': 663, 'from': 287, 'recorded': 587, 'versions': 801, 'songs': 677, 'different': 195, 'languages': 421, '70': 9, 'performers': 543, 'covered': 160, 'since': 661, '1935': 2, 'famous': 256, 'names': 500, 'include': 369, 'billie': 81, 'holiday': 346, 'paul': 536, 'robeson': 604, 'pyotr': 571, 'leschenko': 429, 'russian': 617, 'under': 782, 'title': 752, 'mratschnoje': 491, 'woskresenje': 848, 'bjork': 83, 'sarah': 623, 'mclachlan': 468, 'really': 583, 'got': 309, 'shiver': 652, 'diamanda': 191, 'galás': 292, 'greek': 314, 'born': 91, 'american': 30, 'singer': 662, 'performer': 542, 'voice': 807, 'power': 556, 'her': 338, 'singing': 664, 'described': 181, 'capable': 114, 'unnerving': 789, 'vocal': 806, 'terror': 725, 'she': 650, 'mostly': 484, 'concentrates': 146, 'topics': 761, 'suffering': 701, 'despair': 185, 'condemnation': 149, 'injustice': 373, 'loss': 449, 'sings': 665, 've': 799, 'belonged': 74, 'heroines': 340, 'ancient': 34, 'greece': 313, 'leaves': 426, 'hope': 349, 'grief': 315, 'lost': 450, 'forever': 279, 'unbearable': 778, 'incomparable': 371, 'heights': 337, '10': 0, 'ridiculous': 600, 'plot': 550, 'completely': 142, 'unbelievable': 779, 'woman': 838, 'bets': 77, 'boss': 92, 'lose': 448, 'guy': 319, 'ten': 722, 'fall': 254, 'him': 342, 'go': 304, 'wrong': 852, 'showed': 655, 'catty': 120, 'stupid': 696, 'reason': 584, 'why': 834, 'granted': 311, 'doing': 206, 'idiotic': 360, 'purpose': 570, 'finding': 271, 'lying': 454, 'each': 216, 'other': 524, 'forgiven': 281, 'climax': 132, 'public': 568, 'typical': 777, 'hollywood': 347, 'upset': 793, 'again': 22, 'any': 41, 'talking': 718, 'line': 436, 'curious': 170, 'ears': 220, 'kate': 407, 'hudson': 354, 'matthew': 465, 'mcconaughey': 467, 'quite': 574, 'handsome': 324, 'costumes': 155, 'especially': 238, 'yellow': 854, 'dress': 212, 'wears': 821, 'gorgeous': 308, 'andie': 38, 'anderson': 36, 'benjamin': 75, 'barry': 58, 'talents': 717, 'stinks': 690, 'matter': 464, 'talented': 716, 'cannot': 113, 'save': 624, 'sinking': 667, 'ship': 651, 'sure': 706, 'several': 648, 'people': 538, 'will': 835, 'fine': 272, 'everyone': 243, 'tastes': 719, 'pride': 562, 'myself': 498, 'being': 69, 'sensible': 641, 'kind': 415, 'wastes': 816, 'quality': 572, 'else': 226, 'mind': 473, 'killing': 414, 'wasting': 817, 'hard': 327, 'money': 480, 'may': 466, 'sucker': 700, 'family': 255, 'rebelling': 585, 'mtv': 492, 'faithful': 253, 'care': 115, 'exterior': 248, 'drugs': 214, 'sex': 649, 'crude': 167, 'humor': 355, 'unsympathetic': 790, 'underdeveloped': 783, 'give': 299, 'nothing': 510, 'bothersome': 95, 'nobody': 507, 'learns': 425, 'brief': 98, 'ambivalent': 29, 'references': 589, 'life': 434, 'guts': 318, 'develop': 189, 'critiques': 166, 'guess': 316, 'pathetic': 535, 'every': 242, 'character': 124, 'worse': 846, 'distinguish': 203, 'themselves': 731, 'vegas': 800, 'hardly': 328, 'differ': 194, 'convenience': 152, 'store': 691, 'workers': 843, 'same': 621, 'waste': 815, 'uninspiring': 786, 'forgettable': 280, 'slop': 668, 'waiting': 809}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkP1f-a6NQjd",
        "outputId": "fb03f6ce-742b-431f-bf96-a47ab7a665dc"
      },
      "source": [
        "# ４．フレーズごとの単語の出現回数　&　５.ベクトル化\n",
        "pd.DataFrame(X.toarray(), columns=vec_tfidf.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>10</th>\n",
              "      <th>1930s</th>\n",
              "      <th>1935</th>\n",
              "      <th>1949</th>\n",
              "      <th>1968</th>\n",
              "      <th>1999</th>\n",
              "      <th>2050s</th>\n",
              "      <th>30</th>\n",
              "      <th>45</th>\n",
              "      <th>70</th>\n",
              "      <th>...</th>\n",
              "      <th>would</th>\n",
              "      <th>write</th>\n",
              "      <th>written</th>\n",
              "      <th>wrong</th>\n",
              "      <th>wrote</th>\n",
              "      <th>yellow</th>\n",
              "      <th>york</th>\n",
              "      <th>you</th>\n",
              "      <th>young</th>\n",
              "      <th>your</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.077097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.065524</td>\n",
              "      <td>0.065524</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.042516</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.151750</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.045329</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.038285</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.045329</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.073275</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.200920</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.068313</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.187316</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.052480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.168698</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.03251</td>\n",
              "      <td>0.03251</td>\n",
              "      <td>0.03251</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.03251</td>\n",
              "      <td>0.03251</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.03251</td>\n",
              "      <td>...</td>\n",
              "      <td>0.042188</td>\n",
              "      <td>0.027458</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.03251</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.03251</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.174033</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.053643</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.053643</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.155292</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.034807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047808</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.085320</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.047808</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9 rows × 859 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        10    1930s     1935      1949     1968     1999     2050s        30  \\\n",
              "0  0.00000  0.00000  0.00000  0.000000  0.00000  0.00000  0.000000  0.000000   \n",
              "1  0.00000  0.00000  0.00000  0.000000  0.00000  0.00000  0.000000  0.065524   \n",
              "2  0.00000  0.00000  0.00000  0.000000  0.00000  0.00000  0.045329  0.000000   \n",
              "3  0.00000  0.00000  0.00000  0.000000  0.00000  0.00000  0.000000  0.000000   \n",
              "4  0.00000  0.00000  0.00000  0.000000  0.00000  0.00000  0.000000  0.000000   \n",
              "5  0.00000  0.00000  0.00000  0.168698  0.00000  0.00000  0.000000  0.000000   \n",
              "6  0.03251  0.03251  0.03251  0.000000  0.03251  0.03251  0.000000  0.000000   \n",
              "7  0.00000  0.00000  0.00000  0.000000  0.00000  0.00000  0.000000  0.000000   \n",
              "8  0.00000  0.00000  0.00000  0.000000  0.00000  0.00000  0.000000  0.000000   \n",
              "\n",
              "         45       70  ...     would     write   written     wrong     wrote  \\\n",
              "0  0.000000  0.00000  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "1  0.065524  0.00000  ...  0.042516  0.000000  0.000000  0.000000  0.000000   \n",
              "2  0.000000  0.00000  ...  0.000000  0.000000  0.038285  0.000000  0.045329   \n",
              "3  0.000000  0.00000  ...  0.000000  0.073275  0.000000  0.000000  0.000000   \n",
              "4  0.000000  0.00000  ...  0.000000  0.000000  0.068313  0.000000  0.000000   \n",
              "5  0.000000  0.00000  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "6  0.000000  0.03251  ...  0.042188  0.027458  0.000000  0.000000  0.000000   \n",
              "7  0.000000  0.00000  ...  0.174033  0.000000  0.000000  0.053643  0.000000   \n",
              "8  0.000000  0.00000  ...  0.047808  0.000000  0.000000  0.000000  0.000000   \n",
              "\n",
              "     yellow     york       you    young      your  \n",
              "0  0.000000  0.00000  0.000000  0.00000  0.077097  \n",
              "1  0.000000  0.00000  0.151750  0.00000  0.000000  \n",
              "2  0.000000  0.00000  0.000000  0.00000  0.000000  \n",
              "3  0.000000  0.00000  0.200920  0.00000  0.000000  \n",
              "4  0.000000  0.00000  0.187316  0.00000  0.052480  \n",
              "5  0.000000  0.00000  0.000000  0.00000  0.000000  \n",
              "6  0.000000  0.03251  0.000000  0.03251  0.000000  \n",
              "7  0.053643  0.00000  0.155292  0.00000  0.034807  \n",
              "8  0.000000  0.00000  0.085320  0.00000  0.047808  \n",
              "\n",
              "[9 rows x 859 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEm6zklkNQjd"
      },
      "source": [
        "### 【問題3】TF-IDFを用いた学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gvqcMuWNQjd"
      },
      "source": [
        "X = pd.DataFrame(X.toarray(), columns=vec_tfidf.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vNZ3rwHNQje",
        "outputId": "6f0f598b-9159-45bd-e895-927b4746d7c6"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9, 859)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JJKNq_kNQje"
      },
      "source": [
        "y = []\n",
        "for i in range(len(X)):\n",
        "    y.append(y_train[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L42E76LUNQjf"
      },
      "source": [
        "y = pd.DataFrame(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGqear_2NQjf"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train_train, X_train_val, y_train_train, y_train_val = train_test_split(X, y, test_size = 0.3, random_state = 43)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HGZwteGNQjf",
        "outputId": "d3d7802a-ec27-4052-d4ff-961fc57565a7"
      },
      "source": [
        "# ランダムフォレストによる分類器で学習\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier() # ランダムフォレストモデルのインスタンスを作成\n",
        "rf.fit(X_train_train, y_train_train) # ランダムフォレストモデルの重みを学習"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<ipython-input-13-8f6fd8adce3c>:4: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  rf.fit(X_train_train, y_train_train) # ランダムフォレストモデルの重みを学習\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier()"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBH8DpAdNQjf"
      },
      "source": [
        "y_pred = rf.predict(X_train_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j3PY6iUNQjg",
        "outputId": "ecd5ea06-0b89-4aa0-c799-cbe011480cf5"
      },
      "source": [
        "print(\"予測\")\n",
        "print(y_pred.reshape(-1,1))\n",
        "print(\"実測\")\n",
        "print(y_train_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "予測\n",
            "[[2]\n",
            " [2]\n",
            " [2]]\n",
            "実測\n",
            "   0\n",
            "3  1\n",
            "8  2\n",
            "6  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5g4W2FhNQjg"
      },
      "source": [
        "### 【問題4】TF-IDFのスクラッチ実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOHXqX1sNQjg",
        "outputId": "9dabd99a-03b6-4903-d1a0-6e5f4af568db"
      },
      "source": [
        "# 後で、この出力結果を問題４にも使用する\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = []\n",
        "s_cnt = 0\n",
        "i = 0\n",
        "while s_cnt < 5000:\n",
        "    corpus.append(x_train[i])\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    X = vectorizer.fit_transform(corpus) # １.corpusが元のフレーズ\n",
        "    s_cnt += len(vectorizer.get_feature_names()) # ２.フレーズを分割、３.単語の列を作成\n",
        "    i = i + 1\n",
        "print(s_cnt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGc8xZn7NQjh"
      },
      "source": [
        "import re\n",
        "bow3 = ''\n",
        "all_bows3 = []\n",
        "for i in range(10):\n",
        "    x_train2 = x_train[i].lower() #大文字を小文字に変換する。【問題5】コーパスの前処理をしておく\n",
        "    x_train2 = re.sub(r'\\W',' ', x_train2) # アルファベット、アンダーバー、数字以外の文字を空白に置き換える。\n",
        "    x_train2 = re.sub(r'\\s+',' ',x_train2) # 垂直タブ以外のすべての空白文字が1回以上繰り返す場合、1個の空白に置き換える。\n",
        "    bow3 += x_train2\n",
        "bow3 = bow3.split(' ')\n",
        "all_bows3 = np.array(bow3).reshape(-1)\n",
        "all_bows3 = np.unique(all_bows3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBJaPfx8NQjh",
        "outputId": "0f50e88c-6279-48b2-9661-530566f9634a"
      },
      "source": [
        "all_bows3.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(894,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj_DNu6ENQjh"
      },
      "source": [
        "# ４.単語ごとの出現回数\n",
        "all_bows_count3 = []\n",
        "for j in range(len(all_bows3)):\n",
        "    all_bows_count3.append((all_bows3[j], x_train[0].count(all_bows3[j]), x_train[1].count(all_bows3[j]), x_train[2].count(all_bows3[j]), \n",
        "                            x_train[3].count(all_bows3[j]), x_train[4].count(all_bows3[j]), x_train[5].count(all_bows3[j]), \n",
        "                            x_train[6].count(all_bows3[j]), x_train[7].count(all_bows3[j]), x_train[8].count(all_bows3[j]), \n",
        "                            x_train[9].count(all_bows3[j])))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZXKI2E7NQji",
        "outputId": "cd2acd94-cfcf-4100-8de5-0bd0f51d99c1"
      },
      "source": [
        "pd.DataFrame(all_bows_count3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td></td>\n",
              "      <td>482</td>\n",
              "      <td>1193</td>\n",
              "      <td>1907</td>\n",
              "      <td>808</td>\n",
              "      <td>941</td>\n",
              "      <td>837</td>\n",
              "      <td>2985</td>\n",
              "      <td>1589</td>\n",
              "      <td>1013</td>\n",
              "      <td>331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>10this</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1930s</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1935</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1949</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>889</th>\n",
              "      <td>yellow</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>890</th>\n",
              "      <td>york</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>891</th>\n",
              "      <td>you</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>892</th>\n",
              "      <td>young</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>893</th>\n",
              "      <td>your</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>894 rows × 11 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0    1     2     3    4    5    6     7     8     9    10\n",
              "0            482  1193  1907  808  941  837  2985  1589  1013  331\n",
              "1    10this    0     0     0    0    0    0     0     0     0    0\n",
              "2     1930s    0     0     0    0    0    0     1     0     0    0\n",
              "3      1935    0     0     0    0    0    0     1     0     0    0\n",
              "4      1949    0     0     0    0    0    2     0     0     0    0\n",
              "..      ...  ...   ...   ...  ...  ...  ...   ...   ...   ...  ...\n",
              "889  yellow    0     0     0    0    0    0     0     1     0    0\n",
              "890    york    0     0     0    0    0    0     0     0     0    0\n",
              "891     you    1     4     0    3    4    0     1     6     3    0\n",
              "892   young    0     0     0    0    0    0     1     0     0    0\n",
              "893    your    1     0     0    0    1    0     0     1     1    0\n",
              "\n",
              "[894 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxbfsZxsNQji"
      },
      "source": [
        "# TF-IDF(索引語頻度逆文書頻度)を求める : TF * IDF\n",
        "# TF:指定単語の出現頻度 = 文書内の指定単語の出現回数 / 文書内の全単語の出現回数\n",
        "# IDF:指定単語のレア度 = log(総文書数 / 指定単語を含む文書数)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um3uyo8CNQjj"
      },
      "source": [
        "all_bows_count3 = np.array(all_bows_count3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x413AWA2NQjj",
        "outputId": "6a52ea14-2dce-4b91-bb73-29f110e5e191"
      },
      "source": [
        "all_bows_count3.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(894, 11)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqHMKLS2NQjj"
      },
      "source": [
        "# TF, IDFの初期設定\n",
        "TF = np.zeros((all_bows_count3.shape[0], all_bows_count3.shape[1]))\n",
        "IDF = np.zeros((all_bows_count3.shape[0], all_bows_count3.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2vz5EXMNQjj"
      },
      "source": [
        "# 各フレーズのTFを計算\n",
        "for i in range(1, all_bows_count3.shape[1]):\n",
        "    for j in range(all_bows_count3.shape[0]):\n",
        "        TF[j][i]  = all_bows_count3[j][i].astype(int) / np.sum(all_bows_count3[:,i].astype(int))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnAJyn-oNQjk"
      },
      "source": [
        "# 指定単語ごとのIDFを計算\n",
        "for i in range(all_bows_count3.shape[0]):\n",
        "    w_count = 0\n",
        "    for j in range(1, all_bows_count3.shape[1]):\n",
        "        if all_bows_count3[i][j].astype(int) != 0:\n",
        "            w_count += 1\n",
        "    if w_count == 0:\n",
        "        w_count +=0.001 # なぜか指定単語を含む文書数=0の文字列があるため、ゼロdivide防止のため、0.001を挿入\n",
        "    IDF[i] = np.log((all_bows_count3.shape[1] - 1) / w_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lA3rp9UNQjk",
        "outputId": "6ac153ca-a1d5-4b47-df79-4a25abc49887"
      },
      "source": [
        "# TFとIDFを掛け合わせる。\n",
        "TF_IDF = TF * IDF\n",
        "TF_IDF"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.00048199, 0.0007605 , ..., 0.00085024, 0.00065807,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.00123823, 0.        , ..., 0.00036404, 0.00056352,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 163
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8G1H1ykNQjk"
      },
      "source": [
        "### 【問題5】コーパスの前処理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u-wdmvyNQjk"
      },
      "source": [
        "import re\n",
        "x_train2 = x_train[i].lower() #大文字を小文字に変換する\n",
        "x_train2 = re.sub(r'\\W',' ', x_train2) # アルファベット、アンダーバー、数字以外の文字を空白に置き換える。\n",
        "x_train2 = re.sub(r'\\s+',' ',x_train2) # 垂直タブ以外のすべての空白文字が1回以上繰り返す場合、1個の空白に置き換える。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHu-vB-vNQjl",
        "outputId": "164e0b6a-5c23-47ee-9df4-70d2595ade80"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Full of (then) unknown actors TSF is a great big cuddly romp of a film.<br /><br />The idea of a bunch of bored teenagers ripping off the local sink factory is odd enough, but add in the black humour that Forsyth & Co are so good at and your in for a real treat.<br /><br />The comatose van driver by itself worth seeing, and the canal side chase is just too real to be anything but funny.<br /><br />And for anyone who lived in Glasgow it\\'s a great \"Oh I know where that is\" film.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FvaRlfRNQjl",
        "outputId": "3d8658d1-149e-43fa-f1f4-074390efae39"
      },
      "source": [
        "x_train2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'full of then unknown actors tsf is a great big cuddly romp of a film br br the idea of a bunch of bored teenagers ripping off the local sink factory is odd enough but add in the black humour that forsyth co are so good at and your in for a real treat br br the comatose van driver by itself worth seeing and the canal side chase is just too real to be anything but funny br br and for anyone who lived in glasgow it s a great oh i know where that is film '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AkN4t5TNQjl"
      },
      "source": [
        "### 【問題6】Word2Vecの学習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTQ3D_TONQjl",
        "outputId": "fd9a89dd-dfeb-41c0-941f-81702c2b7211"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.0.1-cp38-cp38-win_amd64.whl (23.9 MB)\n",
            "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\es\\anaconda3\\lib\\site-packages (from gensim) (1.5.2)\n",
            "Requirement already satisfied: Cython==0.29.21 in c:\\users\\es\\anaconda3\\lib\\site-packages (from gensim) (0.29.21)\n",
            "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\es\\anaconda3\\lib\\site-packages (from gensim) (1.19.2)\n",
            "Collecting smart-open>=1.8.1\n",
            "  Downloading smart_open-5.0.0-py3-none-any.whl (56 kB)\n",
            "Installing collected packages: smart-open, gensim\n",
            "Successfully installed gensim-4.0.1 smart-open-5.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxD0_yT1NQjm",
        "outputId": "34fccf48-2a47-4125-8332-00738444e99f"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "model = Word2Vec(min_count=1, vector_size=10, max_vocab_size=1000) # 次元数を10に設定\n",
        "model.build_vocab(all_bows3) # 準備\n",
        "model.train(all_bows3, total_examples=model.corpus_count, epochs=model.epochs) # 学習"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4622, 26615)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oPdcxoHNQjm",
        "outputId": "a26e4006-e180-46a0-a9fa-e9f847e2e179"
      },
      "source": [
        "print(\"語彙の一覧 : {}\".format(model.wv.index_to_key))\n",
        "for vocab in model.wv.index_to_key:\n",
        "    print(\"{}のベクトル : \\n{}\".format(vocab, model.wv[vocab]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "語彙の一覧 : ['e', 'i', 's', 'r', 'n', 't', 'a', 'o', 'l', 'c', 'd', 'u', 'h', 'g', 'm', 'p', 'y', 'b', 'f', 'w', 'v', 'k', 'j', 'x', '9', '0', '1', '5', '3', 'q', 'z', '8', '4', '7', 'á', '2', '6', 'ö', 'é', 'ü']\n",
            "eのベクトル : \n",
            "[-0.13547948 -0.0624392  -0.08455125 -0.14096268  0.2036106   0.1251401\n",
            "  0.8396069   0.60219246 -0.663359    0.3066738 ]\n",
            "iのベクトル : \n",
            "[-0.08382652 -0.09145691 -0.20257594 -0.20387606  0.30195278  0.21027318\n",
            "  0.93280303  0.6104743  -0.8013299   0.3126492 ]\n",
            "sのベクトル : \n",
            "[-0.04702578 -0.00603477 -0.05651702 -0.20568937  0.3461376   0.14642026\n",
            "  0.69283426  0.5283928  -0.63186634  0.2813262 ]\n",
            "rのベクトル : \n",
            "[-0.18619178 -0.06569354 -0.01746122 -0.27076668  0.23845482  0.14995667\n",
            "  0.74927384  0.38454935 -0.5291256   0.25184512]\n",
            "nのベクトル : \n",
            "[-0.22629571 -0.01645539 -0.22435418 -0.2680216   0.2952995   0.19068922\n",
            "  0.6894727   0.6019194  -0.55293214  0.4359195 ]\n",
            "tのベクトル : \n",
            "[-0.19282734 -0.01087951 -0.15924288 -0.19105205  0.3456806   0.12379518\n",
            "  0.71270365  0.38181412 -0.5675674   0.39778265]\n",
            "aのベクトル : \n",
            "[-0.12201997 -0.05363131 -0.15452802 -0.27033818  0.23550697  0.19150455\n",
            "  0.6568456   0.4975784  -0.55736935  0.3153881 ]\n",
            "oのベクトル : \n",
            "[-0.05583366  0.02646314 -0.13107093 -0.29546303  0.3067818   0.17859893\n",
            "  0.7657398   0.45382947 -0.5733752   0.21633047]\n",
            "lのベクトル : \n",
            "[-0.11327864 -0.02104284 -0.05301876 -0.11695971  0.18733129  0.17327732\n",
            "  0.67761135  0.3643986  -0.52625823  0.35054243]\n",
            "cのベクトル : \n",
            "[-0.07891515 -0.04836862 -0.06331247 -0.16202524  0.3112751   0.19332837\n",
            "  0.45961222  0.29384738 -0.4241071   0.31836146]\n",
            "dのベクトル : \n",
            "[-0.17276151 -0.00211389 -0.03095969 -0.08612011  0.26443824  0.05995754\n",
            "  0.49000472  0.38103855 -0.40924233  0.15507433]\n",
            "uのベクトル : \n",
            "[-0.10912328 -0.13304517 -0.15959431 -0.11922467  0.2820793   0.23129056\n",
            "  0.6899551   0.48941034 -0.53095835  0.27446228]\n",
            "hのベクトル : \n",
            "[-0.07153205 -0.09164712 -0.01346794 -0.25965378  0.27859664  0.16665566\n",
            "  0.48650467  0.40235072 -0.44046754  0.31369472]\n",
            "gのベクトル : \n",
            "[ 0.00318144 -0.09048371 -0.10327718 -0.22689448  0.25941184  0.17077936\n",
            "  0.6258159   0.43209997 -0.35071877  0.35202402]\n",
            "mのベクトル : \n",
            "[ 0.00908874 -0.1055251  -0.16428953 -0.1257891   0.13089079  0.18317062\n",
            "  0.4731992   0.26020485 -0.31468162  0.20957626]\n",
            "pのベクトル : \n",
            "[ 0.00118369 -0.13535674 -0.08948068 -0.10193665  0.16839539  0.20111106\n",
            "  0.55872625  0.37782255 -0.38095495  0.29177454]\n",
            "yのベクトル : \n",
            "[-0.14912343  0.00192275 -0.10696968 -0.15729624  0.17731766  0.14549838\n",
            "  0.43894553  0.18034254 -0.21020982  0.22645031]\n",
            "bのベクトル : \n",
            "[-0.10911061 -0.12053573 -0.01725685 -0.0618563   0.15588358  0.03727612\n",
            "  0.3491261   0.2556412  -0.28517535  0.15410258]\n",
            "fのベクトル : \n",
            "[-1.7727512e-01 -1.8672399e-04 -3.4584258e-02 -2.1022262e-01\n",
            "  2.0182498e-01  9.9343009e-02  4.8419398e-01  2.2108588e-01\n",
            " -3.5846031e-01  1.7175202e-01]\n",
            "wのベクトル : \n",
            "[-0.04922196 -0.04546509 -0.03899745 -0.18651077  0.11910354  0.11842359\n",
            "  0.4059165   0.2136658  -0.37814704  0.2109198 ]\n",
            "vのベクトル : \n",
            "[-0.05224543  0.00641517 -0.12165934 -0.11039821  0.20010239  0.15293053\n",
            "  0.2827239   0.23947349 -0.33672962  0.20468588]\n",
            "kのベクトル : \n",
            "[-0.10099803 -0.05903521  0.03865874 -0.0859305   0.20171845  0.02018549\n",
            "  0.3690529   0.31205446 -0.33695713  0.09307206]\n",
            "jのベクトル : \n",
            "[-0.11292312 -0.06913104 -0.07821371 -0.15506501  0.17296249  0.01111304\n",
            "  0.32290682  0.20804128 -0.25235778  0.14596567]\n",
            "xのベクトル : \n",
            "[ 0.03249002 -0.08658764 -0.09886754 -0.06738225 -0.03092029  0.02056232\n",
            "  0.13893034  0.13012198 -0.10152782  0.0562207 ]\n",
            "9のベクトル : \n",
            "[-0.12804924 -0.01856733 -0.07728628 -0.05901283  0.20892775  0.15581352\n",
            "  0.44491404  0.25727105 -0.38772857  0.178447  ]\n",
            "0のベクトル : \n",
            "[-2.3482654e-02 -4.6734545e-02 -1.0767463e-01 -1.4517638e-01\n",
            " -3.1680053e-03 -2.2214570e-04  2.1013328e-01  1.0520640e-01\n",
            " -2.4672735e-01  6.6049017e-02]\n",
            "1のベクトル : \n",
            "[-0.00322747 -0.06178849  0.0382968  -0.06772334  0.03554467  0.16563384\n",
            "  0.3573042   0.2407545  -0.29212356  0.1823658 ]\n",
            "5のベクトル : \n",
            "[ 0.01745249  0.04003019  0.01985585 -0.01957825  0.01961339  0.11761053\n",
            "  0.22974882  0.12579302 -0.13555917  0.0606415 ]\n",
            "3のベクトル : \n",
            "[-0.01276733 -0.0965884  -0.10666328 -0.04767823  0.07020345 -0.0199947\n",
            "  0.09935349  0.02346175 -0.03261819  0.06815168]\n",
            "qのベクトル : \n",
            "[-0.05982289  0.05017552  0.07794081 -0.06543832  0.11248327  0.07419669\n",
            "  0.14289212  0.06066325  0.01580197 -0.03255968]\n",
            "zのベクトル : \n",
            "[-0.09727528  0.08480988 -0.01834748 -0.04666179  0.08116534 -0.01777154\n",
            "  0.11836736  0.13098717 -0.0109597  -0.03404444]\n",
            "8のベクトル : \n",
            "[ 0.08476545  0.04224316  0.03092646 -0.07777873  0.10482231 -0.0081209\n",
            "  0.14000595 -0.01922031 -0.12315851  0.09243951]\n",
            "4のベクトル : \n",
            "[ 0.00492072 -0.02852556  0.08221003  0.07258574 -0.06925979  0.04420812\n",
            "  0.13644218  0.08819018 -0.03944762  0.03681846]\n",
            "7のベクトル : \n",
            "[ 0.00273319 -0.04022129 -0.07589022 -0.02787719  0.04783526  0.0942135\n",
            "  0.11600395 -0.04446129 -0.11245479  0.10832603]\n",
            "áのベクトル : \n",
            "[ 0.02109488  0.04581494  0.0515386  -0.0482266   0.10018226  0.04449187\n",
            "  0.09915507  0.02322438 -0.08800194  0.00680857]\n",
            "2のベクトル : \n",
            "[ 0.03569165 -0.00352985 -0.10259404 -0.109194   -0.04509353  0.01007371\n",
            "  0.06426243  0.12291099  0.01994481 -0.02335046]\n",
            "6のベクトル : \n",
            "[-0.00533628  0.0451565   0.06823041 -0.02697152  0.06400472 -0.0446198\n",
            "  0.04552382  0.117275   -0.06717624 -0.07234772]\n",
            "öのベクトル : \n",
            "[ 0.05066171 -0.04253909 -0.01933651  0.08820476 -0.00711048 -0.03749643\n",
            " -0.01291244  0.09611131 -0.08878836  0.03876857]\n",
            "éのベクトル : \n",
            "[-0.05752246  0.05907434  0.03982238  0.01702332 -0.00357564 -0.02017289\n",
            "  0.12095248  0.07275022 -0.04706074 -0.08248668]\n",
            "üのベクトル : \n",
            "[-0.07820481 -0.01219715 -0.01171423 -0.0322904   0.10518641  0.00075295\n",
            "  0.07951936 -0.06096071 -0.04131814 -0.04660663]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GeUSqtgNQjm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}